{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW01: DATA COLLLECTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Assignment 01 for the course \"Introduction to Data Science\" at the Faculty of Information Technology, University of Science, Vietnam National University, Ho Chi Minh City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Latest update: 09/08/2024)\n",
    "\n",
    "Student Name: Võ Nguyễn Phương Quỳnh\n",
    "\n",
    "Student ID: 22127360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assignment Objectives**\n",
    "\n",
    "By completing this assignment, students will achieve the following objectives:\n",
    "- Understand HTML Parsing\n",
    "- Data Crawling via APIs\n",
    "- Construct Dataframes\n",
    "- Error Handling and Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to Complete and Submit the Assignment**\n",
    "\n",
    "&#9889; **Note**: You should follow the instructions below. If anything is unclear, you need to contact the teaching assistant or instructor immediately for timely support.\n",
    "\n",
    "**How to Do the Assignment**\n",
    "\n",
    "You will work directly on this notebook file. First, fill in your full name and student ID (MSSV) in the header section of the file above. In the file, complete the tasks in sections marked:\n",
    "```python\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "```\n",
    "Or for optional code sections:\n",
    "```python\n",
    "# YOUR CODE HERE (OPTION)\n",
    "```\n",
    "For markdown cells, complete the answer in the section marked:\n",
    "```markdown\n",
    "YOUR ANSWER HERE\n",
    "```\n",
    "\n",
    "**How to Submit the Assignment**\n",
    "\n",
    "Before submitting, select `Kernel` -> `Restart Kernel & Run All Cells` if you are using a local environment, or `Runtime -> Restart session` and run all if using Google Colab, to ensure everything works as expected.\n",
    "\n",
    "Next, create a submission folder with the following structure:\n",
    "- Folder named `MSSV` (for example, if your student ID is `1234567`, name the folder `1234567`)\n",
    "    - File `HW01.ipynb` (no need to submit other files)\n",
    "\n",
    "Finally, compress this `MSSV` folder in `.zip` format (not `.rar` or any other format) and submit it via the link on Moodle.\\\n",
    "<font color=red>Please make sure to strictly follow this submission guideline.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Packages\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# YOUR CODE HERE (OPTION)\n",
    "# If you need other support packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Collect data from a website by parsing HTML (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3abb7b059ef46b75a81b2524d57b3e53",
     "grade": false,
     "grade_id": "task-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, you are tasked with collecting data from a website that simulates the sale of Pokémon. We have provided all the necessary URLs in a file called pokemon.txt, which contains a list of links you need to crawl for data. Each link corresponds to a page with details about different Pokémon for sale.\n",
    "\n",
    "Your goal is to extract specific data from each of these pages and compile it into a structured format.\n",
    "\n",
    "**Expected Output:** You need to create a `DataFrame` containing the following columns for each Pokémon:\n",
    "- `SKU`: The unique identifier (ID) for each Pokémon.\n",
    "- `Name`: The name of the Pokémon.\n",
    "- `Price`: The sale price of the Pokémon.\n",
    "- `InStock`: The quantity of this Pokémon currently available in stock.\n",
    "- `Categories`: The category under which the Pokémon is listed (e.g., type, special edition).\n",
    "- `Tags`: Any additional tags or attributes assigned to the Pokémon (e.g., rare, legendary, etc.).\n",
    "\n",
    "**What You Need to Do**:\n",
    "- Implement the `collect_data` function, which takes one input parameter: `course_urls_file`. This file contains all the URLs you need to process, each on a new line.\n",
    "- For each URL in `pokemon.txt`, crawl the webpage and extract the relevant information for all the Pokémon listed on that page.\n",
    "- The data you collect should be organized into a pandas `DataFrame` with the six specified columns (`SKU`, `Name`, `Price`, `InStock`, `Categories`, and `Tags`).\n",
    "- The output DataFrame should resemble the structure of the provided sample file pokemon_example.csv. This sample contains a few examples to help you visualize the expected result format.\n",
    "\n",
    "**Notes:**\n",
    "- **Price Format**: Ensure the price is captured as a numerical value. If there are symbols like \"$\", remove them before storing the price.\n",
    "- **In Stock**: If a Pokémon is out of stock, mark its quantity as 0 in the InStock field.\n",
    "- **Categories & Tags**: Some Pokémon might belong to multiple categories or have multiple tags. Make sure to capture all relevant information and store them as lists or comma-separated strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>InStock</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4391</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>63.0</td>\n",
       "      <td>45</td>\n",
       "      <td>Pokemon, Seed</td>\n",
       "      <td>bulbasaur, Overgrow, Seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7227</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>87.0</td>\n",
       "      <td>142</td>\n",
       "      <td>Pokemon, Seed</td>\n",
       "      <td>ivysaur, Overgrow, Seed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7036</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>105.0</td>\n",
       "      <td>30</td>\n",
       "      <td>Pokemon, Seed</td>\n",
       "      <td>Overgrow, Seed, venusaur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9086</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>48.0</td>\n",
       "      <td>206</td>\n",
       "      <td>Lizard, Pokemon</td>\n",
       "      <td>Blaze, charmander, Lizard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6565</td>\n",
       "      <td>Charmeleon</td>\n",
       "      <td>165.0</td>\n",
       "      <td>284</td>\n",
       "      <td>Flame, Pokemon</td>\n",
       "      <td>Blaze, charmeleon, Flame</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SKU        Name  Price  InStock       Categories  \\\n",
       "0  4391   Bulbasaur   63.0       45    Pokemon, Seed   \n",
       "1  7227     Ivysaur   87.0      142    Pokemon, Seed   \n",
       "2  7036    Venusaur  105.0       30    Pokemon, Seed   \n",
       "3  9086  Charmander   48.0      206  Lizard, Pokemon   \n",
       "4  6565  Charmeleon  165.0      284   Flame, Pokemon   \n",
       "\n",
       "                        Tags  \n",
       "0  bulbasaur, Overgrow, Seed  \n",
       "1    ivysaur, Overgrow, Seed  \n",
       "2   Overgrow, Seed, venusaur  \n",
       "3  Blaze, charmander, Lizard  \n",
       "4   Blaze, charmeleon, Flame  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokemon_example = pd.read_csv('./assets/pokemon_example.csv')\n",
    "pokemon_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc0f6c4a893d56cecaca95a1b7f2afcb",
     "grade": false,
     "grade_id": "cell-d229a1a778f66947",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def collect_data(course_urls_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect data from a list of URLs provided in a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    course_urls_file : str\n",
    "        Path to the file containing URLs (e.g., 'Lab01/pokemon.txt').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the required data fields.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    NotImplementedError\n",
    "        If the function is not implemented yet.\n",
    "    \"\"\"\n",
    "    # Load URLs from file\n",
    "    with open(course_urls_file, 'r') as file:\n",
    "        urls = [line.strip() for line in file]\n",
    "\n",
    "    # Initialize empty lists to store the values of each attribute\n",
    "    sku, names, prices, in_stocks, categories, tags = [], [], [], [], [], []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    count = 0\n",
    "    \n",
    "    for url in urls:\n",
    "        #print(f\"Processing URL: {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        sku1 = soup.find(attrs={\"class\": \"sku\"})\n",
    "        name = soup.find(attrs={\"class\": \"product_title entry-title\"})\n",
    "        price = soup.find(attrs={\"class\": \"price\"})\n",
    "        inStock = soup.find(attrs={\"class\": \"stock in-stock\"})\n",
    "        category = soup.find(attrs={\"class\": \"posted_in\"})\n",
    "        tag = soup.find(attrs={\"class\": \"tagged_as\"})\n",
    "        \n",
    "        sku.append(sku1.get_text())\n",
    "        names.append(name.get_text())\n",
    "        prices.append(price.get_text().split(\"£\")[1])\n",
    "        in_stocks.append(inStock.get_text().split(\" \")[0])\n",
    "        categories.append(category.get_text().split(\": \")[1])\n",
    "        tags.append(tag.get_text().split(\": \")[1])\n",
    "        \n",
    "        count += 1\n",
    "        # print(f\"Processed {count} of {len(urls)}\")\n",
    "        # if count == 10: break\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Create DataFrame with collected data\n",
    "    data = pd.DataFrame({\n",
    "        \"SKU\": sku,\n",
    "        \"Name\": names,\n",
    "        \"Price\": prices,\n",
    "        \"InStock\": in_stocks,\n",
    "        \"Categories\": categories,\n",
    "        \"Tags\": tags\n",
    "    })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5021c627fa342c4fa02518f0f7bbdf43",
     "grade": true,
     "grade_id": "cell-7e3677f4a926ad6c",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "data_pokemon = collect_data(\"./assets/pokemon.txt\")\n",
    "assert data_pokemon.shape == (755, 6), f\"Expected shape (755, 6), got {data_pokemon.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file with name pokemon.csv to grade\n",
    "data_pokemon.to_csv(\"student_pokemon.csv\", sep=',', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Collect data using Web API (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c2ac4b8192bf4ab02b5c3b72d49de5c",
     "grade": false,
     "grade_id": "cell-f582acd2da57b932",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this section, your task is to practice web data crawling using the World Bank API. You will be working with demographic and statistical data provided by the World Bank, covering population, education, health, and other key indicators for all countries from 1960 to 2022.\n",
    "\n",
    "**Selected Indicators:** You will collect data for the following indicators\n",
    "- `SP.POP.TOTL` – Total Population\n",
    "- `SP.POP.TOTL.FE.IN` – Total Female Population\n",
    "- `SP.POP.TOTL.MA.IN` – Total Male Population\n",
    "- `SP.DYN.CBRT.IN` – Birth Rate\n",
    "- `SP.DYN.CDRT.IN` – Death Rate\n",
    "- `SP.DYN.LE00.MA.IN` – Average Life Expectancy (Male)\n",
    "- `SP.DYN.LE00.FE.IN` – Average Life Expectancy (Female)\n",
    "- `SE.PRM.ENRR` – Primary School Enrollment Rate\n",
    "- `SE.TER.ENRR` – High School Enrollment Rate\n",
    "- `SE.PRM.CMPT.ZS` – Primary School Completion Rate\n",
    "- `SE.ADT.1524.LT.ZS` – Literacy Rate (Ages 15-24)\n",
    "\n",
    "**Countries of Interest**: You are required to collect data from the following 7 countries as\n",
    "- United States of America (US)\n",
    "- India (IN)\n",
    "- China (CN)\n",
    "- Japan (JP)\n",
    "- Canada (CA)\n",
    "- Great Britain (GB)\n",
    "- South Africa (ZA)\n",
    "\n",
    "**Task Overview:** You are to implement a data collection function that queries the World Bank API for the specified indicators across these 7 countries. The data will be collected for each year from 1960 to 2022 and stored in a pandas `DataFrame` named `data_countries`.\n",
    "\n",
    "\n",
    "You can expand your work on collecting data (such as collecting data from other countries and other indicators) by reading: https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-api-documentation\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "- Use the based URL: http://api.worldbank.org/v2/\n",
    "- In order to collect data for each indicator of each country, you can use the URL: \"http://api.worldbank.org/v2/countries/{country_code}/indicators/{indicator_code}\"\n",
    "    + `country_code` and `indicator_code` are provided above.\n",
    "    + For example, you can use the following URL to get the `Total population` of Japan: http://api.worldbank.org/v2/countries/jp/indicators/SP.POP.TOTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Birth Rate</th>\n",
       "      <th>Death Rate</th>\n",
       "      <th>Male life expectancy</th>\n",
       "      <th>Female life expectancy</th>\n",
       "      <th>School enrollment, primary</th>\n",
       "      <th>School enrollment, tertiary</th>\n",
       "      <th>Primary completion rate</th>\n",
       "      <th>Literacy rate</th>\n",
       "      <th>Year</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>333287557.0</td>\n",
       "      <td>168266219.0</td>\n",
       "      <td>165021339.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332031554.0</td>\n",
       "      <td>167550001.0</td>\n",
       "      <td>164481553.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>73.5</td>\n",
       "      <td>79.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2021</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331511512.0</td>\n",
       "      <td>167203010.0</td>\n",
       "      <td>164308503.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>10.3</td>\n",
       "      <td>74.2</td>\n",
       "      <td>79.9</td>\n",
       "      <td>100.305793762207</td>\n",
       "      <td>87.5676574707031</td>\n",
       "      <td>100.923667907715</td>\n",
       "      <td>None</td>\n",
       "      <td>2020</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>328329953.0</td>\n",
       "      <td>165599805.0</td>\n",
       "      <td>162730147.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>8.7</td>\n",
       "      <td>76.3</td>\n",
       "      <td>81.4</td>\n",
       "      <td>100.981300354004</td>\n",
       "      <td>87.8887100219727</td>\n",
       "      <td>100.489051818848</td>\n",
       "      <td>None</td>\n",
       "      <td>2019</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>326838199.0</td>\n",
       "      <td>164926348.0</td>\n",
       "      <td>161911851.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>8.678</td>\n",
       "      <td>76.2</td>\n",
       "      <td>81.2</td>\n",
       "      <td>101.256561279297</td>\n",
       "      <td>88.2991790771484</td>\n",
       "      <td>100.092697143555</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>325122128.0</td>\n",
       "      <td>164151818.0</td>\n",
       "      <td>160970309.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>8.638</td>\n",
       "      <td>76.1</td>\n",
       "      <td>81.1</td>\n",
       "      <td>101.821441650391</td>\n",
       "      <td>88.1673889160156</td>\n",
       "      <td>98.8321990966797</td>\n",
       "      <td>None</td>\n",
       "      <td>2017</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>323071755.0</td>\n",
       "      <td>163224028.0</td>\n",
       "      <td>159847727.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>8.493</td>\n",
       "      <td>76.1</td>\n",
       "      <td>81.1</td>\n",
       "      <td>101.362861633301</td>\n",
       "      <td>88.8350524902344</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2016</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>320738994.0</td>\n",
       "      <td>162158414.0</td>\n",
       "      <td>158580581.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>8.44</td>\n",
       "      <td>76.3</td>\n",
       "      <td>81.2</td>\n",
       "      <td>100.299911499023</td>\n",
       "      <td>88.8894119262695</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>318386329.0</td>\n",
       "      <td>161084758.0</td>\n",
       "      <td>157301571.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>8.237</td>\n",
       "      <td>76.5</td>\n",
       "      <td>81.3</td>\n",
       "      <td>99.6733779907227</td>\n",
       "      <td>88.6268692016602</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>316059947.0</td>\n",
       "      <td>160034189.0</td>\n",
       "      <td>156025758.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>8.215</td>\n",
       "      <td>76.4</td>\n",
       "      <td>81.2</td>\n",
       "      <td>99.455436706543</td>\n",
       "      <td>88.7264175415039</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2013</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Population  Female Population  Male Population Birth Rate Death Rate  \\\n",
       "0       333287557.0        168266219.0      165021339.0       None       None   \n",
       "1       332031554.0        167550001.0      164481553.0       11.0       10.4   \n",
       "2       331511512.0        167203010.0      164308503.0       10.9       10.3   \n",
       "3       328329953.0        165599805.0      162730147.0       11.4        8.7   \n",
       "4       326838199.0        164926348.0      161911851.0       11.6      8.678   \n",
       "5       325122128.0        164151818.0      160970309.0       11.8      8.638   \n",
       "6       323071755.0        163224028.0      159847727.0       12.2      8.493   \n",
       "7       320738994.0        162158414.0      158580581.0       12.4       8.44   \n",
       "8       318386329.0        161084758.0      157301571.0       12.5      8.237   \n",
       "9       316059947.0        160034189.0      156025758.0       12.4      8.215   \n",
       "\n",
       "  Male life expectancy Female life expectancy  School enrollment, primary  \\\n",
       "0                 None                    None                       None   \n",
       "1                 73.5                    79.3                       None   \n",
       "2                 74.2                    79.9           100.305793762207   \n",
       "3                 76.3                    81.4           100.981300354004   \n",
       "4                 76.2                    81.2           101.256561279297   \n",
       "5                 76.1                    81.1           101.821441650391   \n",
       "6                 76.1                    81.1           101.362861633301   \n",
       "7                 76.3                    81.2           100.299911499023   \n",
       "8                 76.5                    81.3           99.6733779907227   \n",
       "9                 76.4                    81.2            99.455436706543   \n",
       "\n",
       "  School enrollment, tertiary Primary completion rate Literacy rate  Year  \\\n",
       "0                        None                    None          None  2022   \n",
       "1                        None                    None          None  2021   \n",
       "2            87.5676574707031        100.923667907715          None  2020   \n",
       "3            87.8887100219727        100.489051818848          None  2019   \n",
       "4            88.2991790771484        100.092697143555          None  2018   \n",
       "5            88.1673889160156        98.8321990966797          None  2017   \n",
       "6            88.8350524902344                    None          None  2016   \n",
       "7            88.8894119262695                    None          None  2015   \n",
       "8            88.6268692016602                    None          None  2014   \n",
       "9            88.7264175415039                    None          None  2013   \n",
       "\n",
       "  Country  \n",
       "0     USA  \n",
       "1     USA  \n",
       "2     USA  \n",
       "3     USA  \n",
       "4     USA  \n",
       "5     USA  \n",
       "6     USA  \n",
       "7     USA  \n",
       "8     USA  \n",
       "9     USA  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_countries_examples = pd.read_csv(\"./assets/countries_example.csv\")\n",
    "data_countries_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.worldbank.org/v2/\"\n",
    "COUNTRIES = [\"US\", \"IN\", \"CN\", \"JP\", \"CA\", \"GB\", \"ZA\"]\n",
    "INDICATORS = [\n",
    "    \"SP.POP.TOTL\",\n",
    "    \"SP.POP.TOTL.FE.IN\",\n",
    "    \"SP.POP.TOTL.MA.IN\",\n",
    "    \"SP.DYN.CBRT.IN\",\n",
    "    \"SP.DYN.CDRT.IN\",\n",
    "    \"SP.DYN.LE00.MA.IN\",\n",
    "    \"SP.DYN.LE00.FE.IN\",\n",
    "    \"SE.PRM.ENRR\",\n",
    "    \"SE.TER.ENRR\",\n",
    "    \"SE.PRM.CMPT.ZS\",\n",
    "    \"SE.ADT.1524.LT.ZS\",\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE (option)\n",
    "# If you need other initializations\n",
    "countryMap = {\n",
    "    \"us\": \"United States\",\n",
    "    \"in\": \"India\",\n",
    "    \"cn\": \"China\",\n",
    "    \"jp\": \"Japan\",\n",
    "    \"ca\": \"Canada\",\n",
    "    \"gb\": \"Great Britain\",\n",
    "    \"za\": \"South Africa\",\n",
    "}\n",
    "\n",
    "featureMap = {\n",
    "    \"SP.POP.TOTL\": \"Total Population\",\n",
    "    \"SP.POP.TOTL.FE.IN\": \"Female Population\",\n",
    "    \"SP.POP.TOTL.MA.IN\": \"Male Population\",\n",
    "    \"SP.DYN.CBRT.IN\": \"Birth Rate\",\n",
    "    \"SP.DYN.CDRT.IN\": \"Death Rate\",\n",
    "    \"SP.DYN.LE00.MA.IN\": \"Life Expectancy Male\",\n",
    "    \"SP.DYN.LE00.FE.IN\": \"Life Expectancy Female\",\n",
    "    \"SE.PRM.ENRR\": \"Primary School Enrollment\",\n",
    "    \"SE.TER.ENRR\": \"High School Enrollment\",\n",
    "    \"SE.PRM.CMPT.ZS\": \"Primary Completion Rate\",\n",
    "    \"SE.ADT.1524.LT.ZS\": \"Literacy rate\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a356b3c54013209b19ff954c7550e809",
     "grade": false,
     "grade_id": "cell-d7f9f143f1ba0448",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def collect_data(country_code: str, per_page: int, start_year: int, end_year: int, max_retries: int = 3, delay: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collect data from the World Bank API for a specific country and date range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    country_code : str\n",
    "        The ISO 3166-1 alpha-2 code for the country (e.g., 'US' for the United States).\n",
    "    per_page : int\n",
    "        Number of records to return per page.\n",
    "    start_year : int\n",
    "        The starting year of the data range.\n",
    "    end_year : int\n",
    "        The ending year of the data range.\n",
    "    max_retries : int, optional\n",
    "        Maximum number of retries for API requests in case of server errors (default is 3).\n",
    "    delay : int, optional\n",
    "        Delay (in seconds) between retries (default is 2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the data collected from the API.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the API request fails or if the data is not available.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    df = pd.DataFrame(columns = INDICATORS)\n",
    "    \n",
    "    countrys = []\n",
    "    years = []\n",
    "    added = False\n",
    "    # print(f\"Processing {country_code}\")\n",
    "    \n",
    "    for indication in INDICATORS:\n",
    "        url = f\"{BASE_URL}country/{country_code}/indicator/{indication}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        \n",
    "        countrysXML = soup.find_all('wb:country')\n",
    "        datesXML = soup.find_all('wb:date')\n",
    "        valuesXML = soup.find_all('wb:value')\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        for i in range(len(valuesXML)):\n",
    "            # check if year is in range\n",
    "            if (int(datesXML[i].get_text()) > end_year or int(datesXML[i].get_text()) < start_year): \n",
    "                continue\n",
    "            \n",
    "            if valuesXML[i].get_text() == '':\n",
    "                data.append(None)\n",
    "            else:\n",
    "                data.append(float(valuesXML[i].get_text()))\n",
    "            \n",
    "            if not added:\n",
    "                countrys.append(countrysXML[i].get_text())\n",
    "                years.append(int(datesXML[i].get_text()))\n",
    "                \n",
    "        df[indication] = data\n",
    "        added = True        \n",
    "    \n",
    "    df['Year'] = years\n",
    "    df['Country'] = countrys\n",
    "    \n",
    "    df.columns = [featureMap[col] for col in INDICATORS] + ['Year', 'Country']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_countries_dataset(country_code_list: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a dataset by collecting data for a list of countries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    country_code_list : list of str\n",
    "        List of ISO 3166-1 alpha-2 country codes (e.g., ['US', 'IN', 'CN']).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the combined data for all countries.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If data collection for any country fails.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    data_frames = []\n",
    "\n",
    "    for country_code in country_code_list:\n",
    "        try:\n",
    "            # Collect data for each country and append the result to the list\n",
    "            df = collect_data(country_code=country_code,\n",
    "                              per_page=100, start_year=2000, end_year=2022)\n",
    "            data_frames.append(df)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error collecting data for {country_code}: {e}\")\n",
    "\n",
    "    # Concatenate all collected DataFrames into a single DataFrame\n",
    "    combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34151828566ad4d8873707659e534fc2",
     "grade": true,
     "grade_id": "cell-f67d0a7fbf4ed980",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "data_countries = generate_countries_dataset(COUNTRIES)\n",
    "assert data_countries.shape == (161, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file with name coutries.csv to grade\n",
    "data_countries.to_csv(\"student_countries.csv\", sep=',', encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebed20dd04dcfb177b61df22a00442c8",
     "grade": false,
     "grade_id": "cell-fbf46b6851f99ce7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# 4. Crawl data from Springer Journal (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c98f75c9df10fa036f8f45a8b239c24d",
     "grade": false,
     "grade_id": "cell-4ed04fd81d73eaa7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a Computer Science student conducting research on a specific topic, it's essential to read papers from academic journals.\n",
    "\n",
    "In this assignment, you will work with the journal SN Computer Science. Your task is to crawl and extract detailed information for every paper published in this journal. You can find an overview of the journal on the [main page](https://link.springer.com/journal/42979), which states:\n",
    "```\n",
    "SN Computer Science is a broad-based, hybrid, peer reviewed journal that publishes original research in all the disciplines of computer science including various inter-disciplinary aspects. The journal aims to be a global forum of, for, and by the community and offers:\n",
    "```\n",
    "\n",
    "Using your previous code as a foundation, you need to create a table (dataframe) that includes the following information for each paper:\n",
    "- `Title`: The title of the paper\n",
    "- `Date`: The publication date\n",
    "- `Link`: The URL to the paper\n",
    "- `Authors`: The authors of the paper\n",
    "- `Affiliations`: The affiliations of each author\n",
    "\n",
    "\n",
    "When you are a Computer Science student and going to do research on a specific topic, you need to read paper from journal. \n",
    "\n",
    "In this task, I will give you a journal, SN Computer Science and you need to crawl all information of each paper which published in this journal. You may find an overview of this journal in the  as:\n",
    "\n",
    "\n",
    "Here is an example of the expected output (dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Affiliations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Upgraded Approach for Identifying Partially...</td>\n",
       "      <td>20 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Barman, Abhijit, Saha, Diganta, Pal, Alok Ranjan</td>\n",
       "      <td>Department of Computer Science and Engineering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Comprehensive Review on Deep Learning Techni...</td>\n",
       "      <td>20 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Sagar, Maloth, Vanmathi, C.</td>\n",
       "      <td>School of Computer Science Engineering and Inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hybrid Deep Learning Approach with Feature Eng...</td>\n",
       "      <td>20 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Bouamrane, Amira, Derdour, Makhlouf, Alksas, A...</td>\n",
       "      <td>LIAOA Laboratory, University of Oum El-Bouaghi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roberta and BERT: Revolutionizing Mental Healt...</td>\n",
       "      <td>19 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Chopra, Sonali, Agarwal, Parul, Ahmed, Jawed, ...</td>\n",
       "      <td>Jamia Hamdard, New Delhi, India, University of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An Intelligent Image Encryption Scheme Based o...</td>\n",
       "      <td>17 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Dutta, Toshika, Gupta, Manish</td>\n",
       "      <td>Department of Computer Science and Engineering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Systematic Review on Federated Learning in E...</td>\n",
       "      <td>17 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Mishra, Sambit Kumar, Sahoo, Subham Kumar, Swa...</td>\n",
       "      <td>Department of Computer Science and Engineering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Park-Net: A Deep Model for Early Detection of ...</td>\n",
       "      <td>17 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Bennour, Akram, Mekhaznia, Tahar</td>\n",
       "      <td>LAMIS Laboratory, Echahid Cheikh Larbi Tebessi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Leveraging Deep Embedding Models for Arabic Te...</td>\n",
       "      <td>17 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Ellouze, Samira, Jaoua, Maher</td>\n",
       "      <td>ANLP Research Group, MIRACL Lab., ISIM Gabes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bioanalytical Method Development and Validatio...</td>\n",
       "      <td>17 September 2024</td>\n",
       "      <td>https://link.springer.com/article/10.1007/s429...</td>\n",
       "      <td>Tallam, Anil Kumar, Reddy, Konatham Teja Kumar...</td>\n",
       "      <td>Department of Pharmacy, Shri Venkateshwara Uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title               Date  \\\n",
       "0  An Upgraded Approach for Identifying Partially...  20 September 2024   \n",
       "1  A Comprehensive Review on Deep Learning Techni...  20 September 2024   \n",
       "2  Hybrid Deep Learning Approach with Feature Eng...  20 September 2024   \n",
       "3  Roberta and BERT: Revolutionizing Mental Healt...  19 September 2024   \n",
       "4  An Intelligent Image Encryption Scheme Based o...  17 September 2024   \n",
       "5  A Systematic Review on Federated Learning in E...  17 September 2024   \n",
       "6  Park-Net: A Deep Model for Early Detection of ...  17 September 2024   \n",
       "7  Leveraging Deep Embedding Models for Arabic Te...  17 September 2024   \n",
       "8  Bioanalytical Method Development and Validatio...  17 September 2024   \n",
       "\n",
       "                                                Link  \\\n",
       "0  https://link.springer.com/article/10.1007/s429...   \n",
       "1  https://link.springer.com/article/10.1007/s429...   \n",
       "2  https://link.springer.com/article/10.1007/s429...   \n",
       "3  https://link.springer.com/article/10.1007/s429...   \n",
       "4  https://link.springer.com/article/10.1007/s429...   \n",
       "5  https://link.springer.com/article/10.1007/s429...   \n",
       "6  https://link.springer.com/article/10.1007/s429...   \n",
       "7  https://link.springer.com/article/10.1007/s429...   \n",
       "8  https://link.springer.com/article/10.1007/s429...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0   Barman, Abhijit, Saha, Diganta, Pal, Alok Ranjan   \n",
       "1                        Sagar, Maloth, Vanmathi, C.   \n",
       "2  Bouamrane, Amira, Derdour, Makhlouf, Alksas, A...   \n",
       "3  Chopra, Sonali, Agarwal, Parul, Ahmed, Jawed, ...   \n",
       "4                      Dutta, Toshika, Gupta, Manish   \n",
       "5  Mishra, Sambit Kumar, Sahoo, Subham Kumar, Swa...   \n",
       "6                   Bennour, Akram, Mekhaznia, Tahar   \n",
       "7                      Ellouze, Samira, Jaoua, Maher   \n",
       "8  Tallam, Anil Kumar, Reddy, Konatham Teja Kumar...   \n",
       "\n",
       "                                        Affiliations  \n",
       "0  Department of Computer Science and Engineering...  \n",
       "1  School of Computer Science Engineering and Inf...  \n",
       "2  LIAOA Laboratory, University of Oum El-Bouaghi...  \n",
       "3  Jamia Hamdard, New Delhi, India, University of...  \n",
       "4  Department of Computer Science and Engineering...  \n",
       "5  Department of Computer Science and Engineering...  \n",
       "6  LAMIS Laboratory, Echahid Cheikh Larbi Tebessi...  \n",
       "7  ANLP Research Group, MIRACL Lab., ISIM Gabes, ...  \n",
       "8  Department of Pharmacy, Shri Venkateshwara Uni...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_journal_examples = pd.read_csv(\"./assets/journal_example.csv\")\n",
    "data_journal_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete this task by writing code that scrapes and organizes this information into a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed4b48266199485379aaa2135ba7075c",
     "grade": true,
     "grade_id": "cell-2c2e1708d3eb97c2",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def url_gennerate(): \n",
    "    BASE_URL = \"https://link.springer.com/journal/42979/articles?filterOpenAccess=false&page=\"\n",
    "    \n",
    "    with open(\"article.txt\", \"w\") as file:\n",
    "        for page in range(1, 66):\n",
    "            url = f\"{BASE_URL}{page}\" \n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            heading_tags = soup.select('h3.app-card-open__heading')\n",
    "            for heading in heading_tags:\n",
    "                # Step 4: Find the <a> tag within the current <h3> tag\n",
    "                link_tag = heading.find('a')\n",
    "                \n",
    "                # Step 5: Check if the <a> tag exists and has an href attribute\n",
    "                if link_tag and 'href' in link_tag.attrs:\n",
    "                    # Step 6: Append the href value to the article_links list\n",
    "                    file.write(f\"{link_tag['href']}\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_journal_data(course_urls_file: str) -> pd.DataFrame:\n",
    "    # Load URLs from file\n",
    "    with open(course_urls_file, 'r') as file:\n",
    "        urls = [line.strip() for line in file]\n",
    "\n",
    "    # Initialize empty lists to store the values of each attribute\n",
    "    titles, dates, links, authors, affiliations = [], [], [], [], []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        title = soup.find(attrs={\"class\": \"c-article-title\"})\n",
    "        if (title):\n",
    "            titles.append(title.get_text())\n",
    "        else :\n",
    "            titles.append(None)\n",
    "        \n",
    "        date = soup.find(attrs={\"class\": \"c-article-identifiers__item\"})\n",
    "        while (date)  and (not (\"Published:\" in date.get_text())):\n",
    "            date = date.find_next_sibling()\n",
    "            if not date:\n",
    "                break\n",
    "        if (date):\n",
    "            dates.append(date.get_text().split(\": \")[1])\n",
    "        else:\n",
    "            dates.append(None)\n",
    "            \n",
    "        links.append(url)\n",
    "        \n",
    "        author = soup.find(attrs={\"class\": \"c-article-author-list\"})\n",
    "        if (author):\n",
    "            author = author.find_all('a', attrs={\"data-test\": \"author-name\"})\n",
    "            author = [link.get_text() for link in author]\n",
    "            authors.append(', '.join(author))\n",
    "        else:\n",
    "            authors.append(None)\n",
    "        \n",
    "        affiliation = soup.find(attrs={\"class\": \"c-article-author-affiliation__list\"})\n",
    "        if (affiliation):\n",
    "            affiliation = affiliation.find_all('p', attrs={\"class\": \"c-article-author-affiliation__address\"})\n",
    "            affiliation = [aff.get_text() for aff in affiliation]\n",
    "            affiliations.append(', '.join(affiliation))\n",
    "        else:\n",
    "            affiliations.append(None)\n",
    "        \n",
    "        count += 1\n",
    "        # print(f\"Processed {count} of {len(urls)}\")      \n",
    "        # if count == 100: break  \n",
    "        \n",
    "        \n",
    "    # Create DataFrame with collected data\n",
    "    data = pd.DataFrame({\n",
    "        \"Title\": titles,\n",
    "        \"Date\": dates,\n",
    "        \"Link\": links,\n",
    "        \"Authors\": authors,\n",
    "        \"Affiliations\": affiliations\n",
    "    })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse aticles' url to txt file.\n",
    "url_gennerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function to collect data\n",
    "data_journal = collect_journal_data(\"article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file with name journal.csv to grade\n",
    "data_journal.to_csv(\"student_journal.csv\", sep=',', encoding='utf-8', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
